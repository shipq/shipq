package codegen

import (
	"bytes"
	"fmt"
	"go/format"
	"sort"
	"strings"

	"github.com/shipq/shipq/db/portsql/ddl"
	"github.com/shipq/shipq/db/portsql/migrate"
	"github.com/shipq/shipq/db/portsql/query"
)

// CompiledQuery holds the compiled information for a single query.
type CompiledQuery struct {
	Name       string
	SQL        string
	Params     []ParamInfo
	Results    []ResultInfo
	ReturnType string // "one", "many", or "exec"
}

// ParamInfo describes a query parameter.
type ParamInfo struct {
	Name   string
	GoType string
}

// ResultInfo describes a result column.
type ResultInfo struct {
	Name   string
	GoType string
	// NestedFields holds the fields for JSON aggregation results.
	// When non-empty, this result is a nested type (slice of objects).
	NestedFields []ResultInfo
}

// GenerateSharedTypes generates types.go with shared param/result structs for all dialects.
// This includes both user-defined query types and CRUD types.
func GenerateSharedTypes(queries []CompiledQuery, crudPlan *migrate.MigrationPlan, packageName string, tableOpts map[string]CRUDOptions) ([]byte, error) {
	// Validate queries for duplicate field names before generating code
	for _, q := range queries {
		if err := validateNoDuplicateFields(q); err != nil {
			return nil, err
		}
	}

	var buf bytes.Buffer

	// Collect imports from user-defined queries
	imports := make(map[string]bool)
	for _, q := range queries {
		for _, p := range q.Params {
			if imp := goTypeImport(p.GoType); imp != "" {
				imports[imp] = true
			}
		}
		collectImportsFromResults(q.Results, imports)
	}

	// Collect imports from CRUD tables
	if crudPlan != nil {
		for _, table := range crudPlan.Schema.Tables {
			for _, col := range table.Columns {
				mapping := MapColumnType(col)
				if mapping.NeedsImport != "" {
					imports[mapping.NeedsImport] = true
				}
			}
		}
	}

	// Sort queries by name for deterministic output
	sort.Slice(queries, func(i, j int) bool {
		return queries[i].Name < queries[j].Name
	})

	// Write header
	buf.WriteString("// Code generated by shipq. DO NOT EDIT.\n")
	buf.WriteString(fmt.Sprintf("package %s\n\n", packageName))

	// Write imports if needed
	if len(imports) > 0 {
		buf.WriteString("import (\n")
		importList := make([]string, 0, len(imports))
		for imp := range imports {
			importList = append(importList, imp)
		}
		sort.Strings(importList)
		for _, imp := range importList {
			buf.WriteString(fmt.Sprintf("\t%q\n", imp))
		}
		buf.WriteString(")\n\n")
	}

	// Generate user-defined query types (params and results only, no SQL)
	if len(queries) > 0 {
		buf.WriteString("// ========== User-Defined Query Types ==========\n\n")
		for _, q := range queries {
			// Params struct (if any)
			if len(q.Params) > 0 {
				buf.WriteString(fmt.Sprintf("// %sParams contains the parameters for %s.\n", q.Name, q.Name))
				buf.WriteString(fmt.Sprintf("type %sParams struct {\n", q.Name))
				for _, p := range q.Params {
					fieldName := toPascalCase(p.Name)
					buf.WriteString(fmt.Sprintf("\t%s %s\n", fieldName, p.GoType))
				}
				buf.WriteString("}\n\n")
			}

			// Result struct (if any)
			if len(q.Results) > 0 {
				buf.WriteString(fmt.Sprintf("// %sResult contains a single result row for %s.\n", q.Name, q.Name))
				buf.WriteString(fmt.Sprintf("type %sResult struct {\n", q.Name))
				for _, r := range q.Results {
					fieldName := toPascalCase(r.Name)
					goType := r.GoType
					// Handle nested types (JSON aggregation)
					if len(r.NestedFields) > 0 {
						nestedTypeName := q.Name + fieldName + "Item"
						goType = "[]" + nestedTypeName
					}
					buf.WriteString(fmt.Sprintf("\t%s %s\n", fieldName, goType))
				}
				buf.WriteString("}\n\n")

				// Generate nested structs for JSON aggregation fields
				generateNestedStructs(&buf, q.Name, "", q.Results)
			}
		}
	}

	// Generate CRUD types for each table
	if crudPlan != nil && len(crudPlan.Schema.Tables) > 0 {
		// Sort table names for deterministic output
		tableNames := make([]string, 0, len(crudPlan.Schema.Tables))
		for name := range crudPlan.Schema.Tables {
			tableNames = append(tableNames, name)
		}
		sort.Strings(tableNames)

		for _, tableName := range tableNames {
			table := crudPlan.Schema.Tables[tableName]
			opts := CRUDOptions{}
			if tableOpts != nil {
				opts = tableOpts[tableName]
			}

			analysis := AnalyzeTable(table)
			singular := toSingular(table.Name)
			singularPascal := toPascalCase(singular)

			buf.WriteString(fmt.Sprintf("// ========== %s CRUD Types ==========\n\n", toPascalCase(table.Name)))

			// --- Get ---
			writeGetTypes(&buf, table, analysis, singularPascal, opts)

			// --- List ---
			writeListTypes(&buf, table, analysis, singularPascal, opts)

			// --- Insert ---
			writeInsertTypes(&buf, table, analysis, singularPascal, opts)

			// --- Update ---
			writeUpdateTypes(&buf, table, analysis, singularPascal, opts)

			// --- Delete ---
			writeDeleteTypes(&buf, table, analysis, singularPascal, opts)

			// --- HardDelete (only if table has deleted_at) ---
			if analysis.HasDeletedAt {
				writeHardDeleteTypes(&buf, table, analysis, singularPascal, opts)
			}
		}
	}

	// Format the code
	formatted, err := format.Source(buf.Bytes())
	if err != nil {
		return buf.Bytes(), fmt.Errorf("failed to format generated code: %w", err)
	}

	return formatted, nil
}

// =============================================================================
// CRUD Type Helpers (for GenerateSharedTypes)
// =============================================================================

// writeGetTypes writes the Get param and result types
func writeGetTypes(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions) {
	// Get params - use public_id if available, otherwise id
	buf.WriteString(fmt.Sprintf("// --- Get ---\n\n"))
	buf.WriteString(fmt.Sprintf("// Get%sParams contains parameters for fetching a single %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("type Get%sParams struct {\n", singularPascal))

	if analysis.HasPublicID {
		buf.WriteString("\tPublicID string\n")
	} else {
		buf.WriteString("\tID int64\n")
	}

	// Add scope column if configured
	if opts.ScopeColumn != "" {
		scopeCol := findColumn(table, opts.ScopeColumn)
		if scopeCol != nil {
			mapping := MapColumnType(*scopeCol)
			buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(opts.ScopeColumn), mapping.GoType))
		}
	}

	buf.WriteString("}\n\n")

	// Get result - all result columns
	buf.WriteString(fmt.Sprintf("// Get%sResult contains the result of fetching a single %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("type Get%sResult struct {\n", singularPascal))
	for _, col := range analysis.ResultColumns {
		mapping := MapColumnType(col)
		buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(col.Name), mapping.GoType))
	}
	buf.WriteString("}\n\n")
}

// writeListTypes writes the List param and result types
func writeListTypes(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions) {
	pluralPascal := toPascalCase(table.Name)

	// Check if table supports cursor pagination (requires both created_at and public_id)
	supportsCursor := analysis.HasCreatedAt && analysis.HasPublicID

	buf.WriteString(fmt.Sprintf("// --- List ---\n\n"))

	if supportsCursor {
		// Generate cursor struct for cursor-based pagination
		writeListCursorStruct(buf, pluralPascal)

		// Generate params with cursor fields
		writeListParamsWithCursor(buf, table, analysis, pluralPascal, opts)

		// Generate result wrapper with Items and NextCursor
		writeListResultWithCursor(buf, table, analysis, pluralPascal)

		// Generate item struct with actual columns
		writeListItemStruct(buf, table, analysis, pluralPascal)
	} else {
		// Fallback to offset-based pagination for tables without created_at/public_id
		writeListParamsWithOffset(buf, table, pluralPascal, opts)
		writeListResultDirect(buf, table, analysis, pluralPascal)
	}
}

// writeListCursorStruct generates the cursor struct for pagination
func writeListCursorStruct(buf *bytes.Buffer, pluralPascal string) {
	buf.WriteString(fmt.Sprintf("// List%sCursor represents a pagination cursor.\n", pluralPascal))
	buf.WriteString(fmt.Sprintf("type List%sCursor struct {\n", pluralPascal))
	buf.WriteString("\tCreatedAt time.Time\n")
	buf.WriteString("\tPublicID  string\n")
	buf.WriteString("}\n\n")
}

// writeListParamsWithCursor generates params with cursor-based pagination fields
func writeListParamsWithCursor(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, pluralPascal string, opts CRUDOptions) {
	buf.WriteString(fmt.Sprintf("// List%sParams contains parameters for listing %s.\n", pluralPascal, table.Name))
	buf.WriteString(fmt.Sprintf("type List%sParams struct {\n", pluralPascal))

	// Add scope column if configured
	if opts.ScopeColumn != "" {
		scopeCol := findColumn(table, opts.ScopeColumn)
		if scopeCol != nil {
			mapping := MapColumnType(*scopeCol)
			buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(opts.ScopeColumn), mapping.GoType))
		}
	}

	buf.WriteString("\tLimit         int\n")
	buf.WriteString(fmt.Sprintf("\tCursor        *List%sCursor\n", pluralPascal))
	buf.WriteString("\tCreatedAfter  *time.Time\n")
	buf.WriteString("\tCreatedBefore *time.Time\n")
	buf.WriteString("}\n\n")
}

// writeListResultWithCursor generates result struct wrapping Items and NextCursor
func writeListResultWithCursor(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, pluralPascal string) {
	buf.WriteString(fmt.Sprintf("// List%sResult contains the paginated result.\n", pluralPascal))
	buf.WriteString(fmt.Sprintf("type List%sResult struct {\n", pluralPascal))
	buf.WriteString(fmt.Sprintf("\tItems      []List%sItem\n", pluralPascal))
	buf.WriteString(fmt.Sprintf("\tNextCursor *List%sCursor\n", pluralPascal))
	buf.WriteString("}\n\n")
}

// writeListItemStruct generates the item struct with actual columns
func writeListItemStruct(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, pluralPascal string) {
	buf.WriteString(fmt.Sprintf("// List%sItem contains a single item from a list of %s.\n", pluralPascal, table.Name))
	buf.WriteString(fmt.Sprintf("type List%sItem struct {\n", pluralPascal))
	for _, col := range analysis.ResultColumns {
		mapping := MapColumnType(col)
		buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(col.Name), mapping.GoType))
	}
	buf.WriteString("}\n\n")
}

// writeListParamsWithOffset generates params with offset-based pagination (fallback)
func writeListParamsWithOffset(buf *bytes.Buffer, table ddl.Table, pluralPascal string, opts CRUDOptions) {
	buf.WriteString(fmt.Sprintf("// List%sParams contains parameters for listing %s.\n", pluralPascal, table.Name))
	buf.WriteString(fmt.Sprintf("type List%sParams struct {\n", pluralPascal))

	// Add scope column if configured
	if opts.ScopeColumn != "" {
		scopeCol := findColumn(table, opts.ScopeColumn)
		if scopeCol != nil {
			mapping := MapColumnType(*scopeCol)
			buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(opts.ScopeColumn), mapping.GoType))
		}
	}

	buf.WriteString("\tLimit  int\n")
	buf.WriteString("\tOffset int\n")
	buf.WriteString("}\n\n")
}

// writeListResultDirect generates result struct with columns directly (fallback)
func writeListResultDirect(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, pluralPascal string) {
	buf.WriteString(fmt.Sprintf("// List%sResult contains a single item from a list of %s.\n", pluralPascal, table.Name))
	buf.WriteString(fmt.Sprintf("type List%sResult struct {\n", pluralPascal))
	for _, col := range analysis.ResultColumns {
		mapping := MapColumnType(col)
		buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(col.Name), mapping.GoType))
	}
	buf.WriteString("}\n\n")
}

// writeInsertTypes writes the Insert param type
func writeInsertTypes(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions) {
	buf.WriteString(fmt.Sprintf("// --- Insert ---\n\n"))
	buf.WriteString(fmt.Sprintf("// Insert%sParams contains parameters for inserting a new %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("type Insert%sParams struct {\n", singularPascal))

	// Add scope column if configured (user provides it on insert)
	if opts.ScopeColumn != "" {
		scopeCol := findColumn(table, opts.ScopeColumn)
		if scopeCol != nil {
			mapping := MapColumnType(*scopeCol)
			buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(opts.ScopeColumn), mapping.GoType))
		}
	}

	// Only user-provided columns (not auto-filled)
	for _, col := range analysis.UserColumns {
		// Skip scope column if already added
		if opts.ScopeColumn != "" && col.Name == opts.ScopeColumn {
			continue
		}
		mapping := MapColumnType(col)
		buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(col.Name), mapping.GoType))
	}
	buf.WriteString("}\n\n")
}

// writeUpdateTypes writes the Update param type
func writeUpdateTypes(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions) {
	buf.WriteString(fmt.Sprintf("// --- Update ---\n\n"))
	buf.WriteString(fmt.Sprintf("// Update%sParams contains parameters for updating a %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("type Update%sParams struct {\n", singularPascal))

	// Primary identifier for WHERE clause
	if analysis.HasPublicID {
		buf.WriteString("\tPublicID string\n")
	} else {
		buf.WriteString("\tID int64\n")
	}

	// Add scope column if configured
	if opts.ScopeColumn != "" {
		scopeCol := findColumn(table, opts.ScopeColumn)
		if scopeCol != nil {
			mapping := MapColumnType(*scopeCol)
			buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(opts.ScopeColumn), mapping.GoType))
		}
	}

	// User-provided columns
	for _, col := range analysis.UserColumns {
		// Skip scope column if already added
		if opts.ScopeColumn != "" && col.Name == opts.ScopeColumn {
			continue
		}
		mapping := MapColumnType(col)
		buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(col.Name), mapping.GoType))
	}
	buf.WriteString("}\n\n")
}

// writeDeleteTypes writes the Delete param type
func writeDeleteTypes(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions) {
	buf.WriteString(fmt.Sprintf("// --- Delete ---\n\n"))
	buf.WriteString(fmt.Sprintf("// Delete%sParams contains parameters for deleting a %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("type Delete%sParams struct {\n", singularPascal))
	if analysis.HasPublicID {
		buf.WriteString("\tPublicID string\n")
	} else {
		buf.WriteString("\tID int64\n")
	}
	// Add scope column if configured
	if opts.ScopeColumn != "" {
		scopeCol := findColumn(table, opts.ScopeColumn)
		if scopeCol != nil {
			mapping := MapColumnType(*scopeCol)
			buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(opts.ScopeColumn), mapping.GoType))
		}
	}
	buf.WriteString("}\n\n")
}

// writeHardDeleteTypes writes the HardDelete param type
func writeHardDeleteTypes(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions) {
	buf.WriteString(fmt.Sprintf("// --- HardDelete ---\n\n"))
	buf.WriteString(fmt.Sprintf("// HardDelete%sParams contains parameters for permanently deleting a %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("type HardDelete%sParams struct {\n", singularPascal))
	if analysis.HasPublicID {
		buf.WriteString("\tPublicID string\n")
	} else {
		buf.WriteString("\tID int64\n")
	}
	// Add scope column if configured
	if opts.ScopeColumn != "" {
		scopeCol := findColumn(table, opts.ScopeColumn)
		if scopeCol != nil {
			mapping := MapColumnType(*scopeCol)
			buf.WriteString(fmt.Sprintf("\t%s %s\n", toPascalCase(opts.ScopeColumn), mapping.GoType))
		}
	}
	buf.WriteString("}\n\n")
}

// findColumn finds a column by name in a table
func findColumn(table ddl.Table, name string) *ddl.ColumnDefinition {
	for i := range table.Columns {
		if table.Columns[i].Name == name {
			return &table.Columns[i]
		}
	}
	return nil
}

// collectImportsFromResults recursively collects imports from results including nested fields.
func collectImportsFromResults(results []ResultInfo, imports map[string]bool) {
	for _, r := range results {
		if imp := goTypeImport(r.GoType); imp != "" {
			imports[imp] = true
		}
		// Recursively check nested fields
		if len(r.NestedFields) > 0 {
			collectImportsFromResults(r.NestedFields, imports)
		}
	}
}

// generateNestedStructs generates struct types for JSON aggregation fields.
// It handles multi-level nesting by recursively processing NestedFields.
func generateNestedStructs(buf *bytes.Buffer, queryName, parentPath string, results []ResultInfo) {
	for _, r := range results {
		if len(r.NestedFields) == 0 {
			continue
		}

		fieldName := toPascalCase(r.Name)
		// Build the current path for this type
		var currentPath string
		if parentPath == "" {
			currentPath = fieldName
		} else {
			currentPath = parentPath + fieldName
		}
		typeName := queryName + currentPath + "Item"

		buf.WriteString(fmt.Sprintf("// %s represents a single item in the %s JSON array.\n", typeName, r.Name))
		buf.WriteString(fmt.Sprintf("type %s struct {\n", typeName))
		for _, nested := range r.NestedFields {
			nestedFieldName := toPascalCase(nested.Name)
			goType := nested.GoType
			// Handle deeply nested types - use the full path
			if len(nested.NestedFields) > 0 {
				nestedTypeName := queryName + currentPath + nestedFieldName + "Item"
				goType = "[]" + nestedTypeName
			}
			buf.WriteString(fmt.Sprintf("\t%s %s\n", nestedFieldName, goType))
		}
		buf.WriteString("}\n\n")

		// Recursively generate structs for deeply nested fields
		generateNestedStructs(buf, queryName, currentPath, r.NestedFields)
	}
}

// validateNoDuplicateFields checks that a query's result columns don't have duplicate
// field names after PascalCase conversion. This would generate invalid Go code.
func validateNoDuplicateFields(q CompiledQuery) error {
	// Check result fields
	resultFields := make(map[string]string) // PascalCase name -> original name
	for _, r := range q.Results {
		fieldName := toPascalCase(r.Name)
		if existingName, exists := resultFields[fieldName]; exists {
			return fmt.Errorf(
				"query %q has duplicate result field %q (from columns %q and %q). "+
					"Use SelectAs() to give one of them an alias",
				q.Name, fieldName, existingName, r.Name,
			)
		}
		resultFields[fieldName] = r.Name
	}

	// Check param fields
	paramFields := make(map[string]string)
	for _, p := range q.Params {
		fieldName := toPascalCase(p.Name)
		if existingName, exists := paramFields[fieldName]; exists {
			return fmt.Errorf(
				"query %q has duplicate parameter field %q (from params %q and %q)",
				q.Name, fieldName, existingName, p.Name,
			)
		}
		paramFields[fieldName] = p.Name
	}

	return nil
}

// goTypeImport returns the import path needed for a Go type, or empty string if none.
func goTypeImport(goType string) string {
	if strings.Contains(goType, "time.Time") {
		return "time"
	}
	if strings.Contains(goType, "json.RawMessage") {
		return "encoding/json"
	}
	return ""
}

// hasJSONAggFields returns true if any result field is a JSONAgg field (has NestedFields).
func hasJSONAggFields(results []ResultInfo) bool {
	for _, r := range results {
		if len(r.NestedFields) > 0 {
			return true
		}
	}
	return false
}

// formatSQLString formats SQL as a Go string literal.
func formatSQLString(sql string) string {
	// Use backticks for multi-line or complex SQL
	if strings.Contains(sql, "\n") || strings.Contains(sql, `"`) {
		// Escape backticks by using string concatenation
		if strings.Contains(sql, "`") {
			// Fall back to double-quoted string with escaping
			escaped := strings.ReplaceAll(sql, `\`, `\\`)
			escaped = strings.ReplaceAll(escaped, `"`, `\"`)
			escaped = strings.ReplaceAll(escaped, "\n", `\n`)
			return fmt.Sprintf(`"%s"`, escaped)
		}
		return fmt.Sprintf("`%s`", sql)
	}
	// Simple single-line SQL
	return fmt.Sprintf("`%s`", sql)
}

// ExtractResultInfo extracts result column information from a SELECT query AST.
// Returns an error if a SELECT expression cannot have its result name inferred
// (e.g., complex expressions without an alias).
func ExtractResultInfo(ast *query.AST) ([]ResultInfo, error) {
	if ast.Kind != query.SelectQuery {
		return nil, nil
	}

	var results []ResultInfo
	for _, sel := range ast.SelectCols {
		name := ""
		goType := "interface{}"
		var nestedFields []ResultInfo

		// Determine name
		if sel.Alias != "" {
			name = sel.Alias
		} else {
			// Try to extract from expression
			switch e := sel.Expr.(type) {
			case query.ColumnExpr:
				name = e.Column.ColumnName()
				goType = e.Column.GoType()
			case query.AggregateExpr:
				// Aggregates need an alias or we use the function name
				if sel.Alias == "" {
					name = strings.ToLower(string(e.Func))
				}
				// Determine type based on aggregate
				switch e.Func {
				case query.AggCount:
					goType = "int64"
				case query.AggSum, query.AggAvg:
					// Type depends on the argument
					if col, ok := e.Arg.(query.ColumnExpr); ok {
						goType = col.Column.GoType()
					} else {
						goType = "float64"
					}
				case query.AggMin, query.AggMax:
					if col, ok := e.Arg.(query.ColumnExpr); ok {
						goType = col.Column.GoType()
					}
				}
			case query.JSONAggExpr:
				// JSON aggregation - extract nested column types
				name = e.FieldName
				// Extract nested fields from the columns
				for _, col := range e.Columns {
					nestedFields = append(nestedFields, ResultInfo{
						Name:   col.ColumnName(),
						GoType: col.GoType(),
					})
				}
				// GoType will be set by GenerateQueriesPackage based on the nested type name
				goType = "" // Marker - will be replaced with []TypeName
			default:
				// Cannot infer name for this expression type - require an alias
				return nil, fmt.Errorf(
					"cannot infer result name for expression type %T in SELECT clause; "+
						"use SelectExprAs() to provide an alias",
					sel.Expr,
				)
			}
		}

		// Get Go type from column expression
		if goType == "interface{}" {
			switch e := sel.Expr.(type) {
			case query.ColumnExpr:
				goType = e.Column.GoType()
			}
		}

		// Handle JSONAggExpr when it has an alias
		if goType == "interface{}" || goType == "" {
			switch e := sel.Expr.(type) {
			case query.JSONAggExpr:
				for _, col := range e.Columns {
					nestedFields = append(nestedFields, ResultInfo{
						Name:   col.ColumnName(),
						GoType: col.GoType(),
					})
				}
				goType = "" // Marker - will be replaced with []TypeName
			}
		}

		if name != "" {
			results = append(results, ResultInfo{
				Name:         name,
				GoType:       goType,
				NestedFields: nestedFields,
			})
		}
	}

	return results, nil
}

// ExtractParamInfo extracts parameter information from the AST by walking
// all expressions and collecting ParamExpr instances.
func ExtractParamInfo(ast *query.AST) []ParamInfo {
	var params []ParamInfo
	seen := make(map[string]bool)

	// Helper to collect params from an expression
	var collectFromExpr func(expr query.Expr)
	collectFromExpr = func(expr query.Expr) {
		if expr == nil {
			return
		}

		switch e := expr.(type) {
		case query.ParamExpr:
			if !seen[e.Name] {
				params = append(params, ParamInfo{
					Name:   e.Name,
					GoType: e.GoType,
				})
				seen[e.Name] = true
			}
		case query.BinaryExpr:
			collectFromExpr(e.Left)
			collectFromExpr(e.Right)
		case query.UnaryExpr:
			collectFromExpr(e.Expr)
		case query.FuncExpr:
			for _, arg := range e.Args {
				collectFromExpr(arg)
			}
		case query.ListExpr:
			for _, val := range e.Values {
				collectFromExpr(val)
			}
		case query.AggregateExpr:
			collectFromExpr(e.Arg)
		case query.SubqueryExpr:
			collectFromAST(e.Query, &params, seen)
		case query.ExistsExpr:
			collectFromAST(e.Subquery, &params, seen)
		}
	}

	collectFromAST(ast, &params, seen)
	return params
}

// collectFromAST walks an AST and collects all params.
func collectFromAST(ast *query.AST, params *[]ParamInfo, seen map[string]bool) {
	if ast == nil {
		return
	}

	// Helper to collect params from an expression
	var collectFromExpr func(expr query.Expr)
	collectFromExpr = func(expr query.Expr) {
		if expr == nil {
			return
		}

		switch e := expr.(type) {
		case query.ParamExpr:
			if !seen[e.Name] {
				*params = append(*params, ParamInfo{
					Name:   e.Name,
					GoType: e.GoType,
				})
				seen[e.Name] = true
			}
		case query.BinaryExpr:
			collectFromExpr(e.Left)
			collectFromExpr(e.Right)
		case query.UnaryExpr:
			collectFromExpr(e.Expr)
		case query.FuncExpr:
			for _, arg := range e.Args {
				collectFromExpr(arg)
			}
		case query.ListExpr:
			for _, val := range e.Values {
				collectFromExpr(val)
			}
		case query.AggregateExpr:
			collectFromExpr(e.Arg)
		case query.SubqueryExpr:
			collectFromAST(e.Query, params, seen)
		case query.ExistsExpr:
			collectFromAST(e.Subquery, params, seen)
		}
	}

	// Walk SELECT columns
	for _, sel := range ast.SelectCols {
		collectFromExpr(sel.Expr)
	}

	// Walk joins
	for _, join := range ast.Joins {
		collectFromExpr(join.Condition)
	}

	// Walk WHERE
	collectFromExpr(ast.Where)

	// Walk HAVING
	collectFromExpr(ast.Having)

	// Walk ORDER BY
	for _, ob := range ast.OrderBy {
		collectFromExpr(ob.Expr)
	}

	// Walk LIMIT and OFFSET
	collectFromExpr(ast.Limit)
	collectFromExpr(ast.Offset)

	// Walk INSERT values
	for _, val := range ast.InsertVals {
		collectFromExpr(val)
	}

	// Walk SET clauses
	for _, set := range ast.SetClauses {
		collectFromExpr(set.Value)
	}

	// Walk set operations
	if ast.SetOp != nil {
		collectFromAST(ast.SetOp.Left, params, seen)
		collectFromAST(ast.SetOp.Right, params, seen)
	}

	// Walk CTEs
	for _, cte := range ast.CTEs {
		collectFromAST(cte.Query, params, seen)
	}
}

// =============================================================================
// QueryRunner Generation for User-Defined Queries
// =============================================================================

// DialectSQL holds compiled SQL for each database dialect.
type DialectSQL struct {
	Postgres string
	MySQL    string
	SQLite   string
}

// DialectParamOccurrences holds the parameter occurrence order for each dialect.
// This includes duplicates - if a param is used twice in the SQL, it appears twice.
type DialectParamOccurrences struct {
	Postgres []string
	MySQL    []string
	SQLite   []string
}

// CompiledQueryWithDialects extends CompiledQuery with per-dialect SQL.
type CompiledQueryWithDialects struct {
	CompiledQuery
	SQL DialectSQL
	// ParamOccurrences holds the parameter names in occurrence order (with duplicates).
	// This is needed because the SQL placeholders ($1, $2 or ?, ?) are per-occurrence,
	// so we need to build args in the same order.
	ParamOccurrences DialectParamOccurrences
}

// toLowerCamelCase converts a PascalCase string to lowerCamelCase.
func toLowerCamelCase(s string) string {
	if len(s) == 0 {
		return s
	}
	// Handle consecutive uppercase letters at the start
	for i, r := range s {
		if i == 0 {
			continue
		}
		if r >= 'A' && r <= 'Z' {
			continue
		}
		// Found first lowercase
		if i > 1 {
			// Keep all but last uppercase as lowercase
			return strings.ToLower(s[:i-1]) + s[i-1:]
		}
		return strings.ToLower(s[:1]) + s[1:]
	}
	// All uppercase
	return strings.ToLower(s)
}

// =============================================================================
// Dialect-Specific Runner Generation
// =============================================================================

// GenerateDialectRunner generates a runner.go for a specific dialect (sqlite, postgres, mysql).
// The generated code imports the parent package for shared types.
func GenerateDialectRunner(
	queries []CompiledQueryWithDialects,
	crudPlan *migrate.MigrationPlan,
	dialect string, // "sqlite", "postgres", or "mysql"
	typesImportPath string, // e.g., "myapp/queries"
	tableOpts map[string]CRUDOptions,
) ([]byte, error) {
	var buf bytes.Buffer

	// Sort queries by name for deterministic output
	sort.Slice(queries, func(i, j int) bool {
		return queries[i].Name < queries[j].Name
	})

	// Sort CRUD tables by name
	var tableNames []string
	if crudPlan != nil {
		tableNames = make([]string, 0, len(crudPlan.Schema.Tables))
		for name := range crudPlan.Schema.Tables {
			tableNames = append(tableNames, name)
		}
		sort.Strings(tableNames)
	}

	// Collect imports
	imports := map[string]bool{
		"context":      true,
		"database/sql": true,
	}

	// Always import the parent types package if we have queries or CRUD
	if len(queries) > 0 || len(tableNames) > 0 {
		imports[typesImportPath] = true
	}

	// Check if any query has JSONAgg fields (need encoding/json and fmt for unmarshalling)
	for _, q := range queries {
		if hasJSONAggFields(q.Results) {
			imports["encoding/json"] = true
			imports["fmt"] = true
			break
		}
	}

	// nanoid is only needed if we have CRUD tables with public_id (for Insert methods)
	// strings is needed for cursor-based pagination (dynamic SQL building)
	for _, tableName := range tableNames {
		table := crudPlan.Schema.Tables[tableName]
		analysis := AnalyzeTable(table)
		if analysis.HasPublicID {
			imports["github.com/shipq/shipq/nanoid"] = true
		}
		// Cursor pagination requires strings package for dynamic SQL building
		if analysis.HasCreatedAt && analysis.HasPublicID {
			imports["strings"] = true
		}
	}

	// Write package and imports
	buf.WriteString("// Code generated by shipq. DO NOT EDIT.\n")
	buf.WriteString(fmt.Sprintf("package %s\n\n", dialect))

	buf.WriteString("import (\n")
	importList := make([]string, 0, len(imports))
	for imp := range imports {
		importList = append(importList, imp)
	}
	sort.Strings(importList)
	for _, imp := range importList {
		buf.WriteString(fmt.Sprintf("\t%q\n", imp))
	}
	buf.WriteString(")\n\n")

	// Write Querier interface
	buf.WriteString("// Querier is the interface for executing queries.\n")
	buf.WriteString("type Querier interface {\n")
	buf.WriteString("\tExecContext(ctx context.Context, query string, args ...any) (sql.Result, error)\n")
	buf.WriteString("\tQueryContext(ctx context.Context, query string, args ...any) (*sql.Rows, error)\n")
	buf.WriteString("\tQueryRowContext(ctx context.Context, query string, args ...any) *sql.Row\n")
	buf.WriteString("}\n\n")

	// Write QueryRunner struct (no dialect field needed!)
	buf.WriteString(fmt.Sprintf("// QueryRunner for %s.\n", dialect))
	buf.WriteString("type QueryRunner struct {\n")
	buf.WriteString("\tdb Querier\n\n")

	// Add SQL string fields for user-defined queries
	if len(queries) > 0 {
		buf.WriteString("\t// User-defined query SQL strings\n")
		for _, q := range queries {
			fieldName := toLowerCamelCase(q.Name) + "SQL"
			buf.WriteString(fmt.Sprintf("\t%s string\n", fieldName))
		}
		buf.WriteString("\n")
	}

	// Add SQL string fields for CRUD
	for _, tableName := range tableNames {
		table := crudPlan.Schema.Tables[tableName]
		analysis := AnalyzeTable(table)
		singular := toSingular(tableName)

		buf.WriteString(fmt.Sprintf("\t// %s CRUD SQL strings\n", toPascalCase(tableName)))
		buf.WriteString(fmt.Sprintf("\tget%sSQL        string\n", toPascalCase(singular)))
		buf.WriteString(fmt.Sprintf("\tlist%sSQL       string\n", toPascalCase(tableName)))
		buf.WriteString(fmt.Sprintf("\tinsert%sSQL     string\n", toPascalCase(singular)))
		buf.WriteString(fmt.Sprintf("\tupdate%sSQL     string\n", toPascalCase(singular)))
		buf.WriteString(fmt.Sprintf("\tdelete%sSQL     string\n", toPascalCase(singular)))
		if analysis.HasDeletedAt {
			buf.WriteString(fmt.Sprintf("\thardDelete%sSQL string\n", toPascalCase(singular)))
		}
		buf.WriteString("\n")
	}
	buf.WriteString("}\n\n")

	// Get the dialect enum for SQL generation
	var dialectEnum SQLDialect
	switch dialect {
	case "postgres":
		dialectEnum = SQLDialectPostgres
	case "mysql":
		dialectEnum = SQLDialectMySQL
	case "sqlite":
		dialectEnum = SQLDialectSQLite
	}

	// Write NewQueryRunner constructor (no dialect parameter!)
	buf.WriteString(fmt.Sprintf("// NewQueryRunner creates a %s query runner.\n", dialect))
	buf.WriteString("func NewQueryRunner(db Querier) *QueryRunner {\n")
	buf.WriteString("\treturn &QueryRunner{\n")
	buf.WriteString("\t\tdb: db,\n")

	// User-defined query SQL
	for _, q := range queries {
		fieldName := toLowerCamelCase(q.Name) + "SQL"
		var sql string
		switch dialectEnum {
		case SQLDialectPostgres:
			sql = q.SQL.Postgres
		case SQLDialectMySQL:
			sql = q.SQL.MySQL
		case SQLDialectSQLite:
			sql = q.SQL.SQLite
		}
		buf.WriteString(fmt.Sprintf("\t\t%s: %q,\n", fieldName, sql))
	}

	// CRUD SQL
	for _, tableName := range tableNames {
		table := crudPlan.Schema.Tables[tableName]
		opts := CRUDOptions{}
		if tableOpts != nil {
			opts = tableOpts[tableName]
		}
		sqlSet := GenerateCRUDSQL(table, dialectEnum, opts)
		analysis := AnalyzeTable(table)
		singular := toSingular(tableName)

		buf.WriteString(fmt.Sprintf("\t\tget%sSQL: %q,\n", toPascalCase(singular), sqlSet.GetSQL))
		buf.WriteString(fmt.Sprintf("\t\tlist%sSQL: %q,\n", toPascalCase(tableName), sqlSet.ListSQL))
		buf.WriteString(fmt.Sprintf("\t\tinsert%sSQL: %q,\n", toPascalCase(singular), sqlSet.InsertSQL))
		buf.WriteString(fmt.Sprintf("\t\tupdate%sSQL: %q,\n", toPascalCase(singular), sqlSet.UpdateSQL))
		buf.WriteString(fmt.Sprintf("\t\tdelete%sSQL: %q,\n", toPascalCase(singular), sqlSet.DeleteSQL))
		if analysis.HasDeletedAt {
			buf.WriteString(fmt.Sprintf("\t\thardDelete%sSQL: %q,\n", toPascalCase(singular), sqlSet.HardDeleteSQL))
		}
	}

	buf.WriteString("\t}\n")
	buf.WriteString("}\n\n")

	// Write WithTx method
	buf.WriteString("// WithTx returns a new QueryRunner using the given transaction.\n")
	buf.WriteString("func (r *QueryRunner) WithTx(tx *sql.Tx) *QueryRunner {\n")
	buf.WriteString("\treturn &QueryRunner{\n")
	buf.WriteString("\t\tdb: tx,\n")

	// Copy user-defined query SQL fields
	for _, q := range queries {
		fieldName := toLowerCamelCase(q.Name) + "SQL"
		buf.WriteString(fmt.Sprintf("\t\t%s: r.%s,\n", fieldName, fieldName))
	}

	// Copy CRUD SQL fields
	for _, tableName := range tableNames {
		table := crudPlan.Schema.Tables[tableName]
		analysis := AnalyzeTable(table)
		singular := toSingular(tableName)

		buf.WriteString(fmt.Sprintf("\t\tget%sSQL: r.get%sSQL,\n", toPascalCase(singular), toPascalCase(singular)))
		buf.WriteString(fmt.Sprintf("\t\tlist%sSQL: r.list%sSQL,\n", toPascalCase(tableName), toPascalCase(tableName)))
		buf.WriteString(fmt.Sprintf("\t\tinsert%sSQL: r.insert%sSQL,\n", toPascalCase(singular), toPascalCase(singular)))
		buf.WriteString(fmt.Sprintf("\t\tupdate%sSQL: r.update%sSQL,\n", toPascalCase(singular), toPascalCase(singular)))
		buf.WriteString(fmt.Sprintf("\t\tdelete%sSQL: r.delete%sSQL,\n", toPascalCase(singular), toPascalCase(singular)))
		if analysis.HasDeletedAt {
			buf.WriteString(fmt.Sprintf("\t\thardDelete%sSQL: r.hardDelete%sSQL,\n", toPascalCase(singular), toPascalCase(singular)))
		}
	}

	buf.WriteString("\t}\n")
	buf.WriteString("}\n\n")

	// Get the types package name (last part of import path)
	typesPkg := "queries"
	if idx := strings.LastIndex(typesImportPath, "/"); idx >= 0 {
		typesPkg = typesImportPath[idx+1:]
	}

	// Generate methods for user-defined queries
	for _, q := range queries {
		generateDialectQueryMethod(&buf, q, dialect, typesPkg)
	}

	// Generate CRUD methods for each table
	for _, tableName := range tableNames {
		table := crudPlan.Schema.Tables[tableName]
		opts := CRUDOptions{}
		if tableOpts != nil {
			opts = tableOpts[tableName]
		}
		generateDialectCRUDMethods(&buf, table, opts, dialect, typesPkg)
	}

	// Format the code
	formatted, err := format.Source(buf.Bytes())
	if err != nil {
		return buf.Bytes(), fmt.Errorf("failed to format generated code: %w", err)
	}

	return formatted, nil
}

// generateDialectQueryMethod generates a query method with dialect-specific scanning.
func generateDialectQueryMethod(buf *bytes.Buffer, q CompiledQueryWithDialects, dialect, typesPkg string) {
	sqlField := toLowerCamelCase(q.Name) + "SQL"

	// Identify JSON columns that need special handling for SQLite
	jsonColumns := make(map[string]bool)
	for _, r := range q.Results {
		if r.GoType == "json.RawMessage" {
			jsonColumns[toPascalCase(r.Name)] = true
		}
	}

	// Identify JSONAgg fields that need scan-then-unmarshal handling
	// These have NestedFields populated and compile to typed slices
	jsonAggFields := make(map[string]bool)
	for _, r := range q.Results {
		if len(r.NestedFields) > 0 {
			jsonAggFields[toPascalCase(r.Name)] = true
		}
	}

	switch q.ReturnType {
	case "one":
		generateDialectOneMethod(buf, q, sqlField, dialect, typesPkg, jsonColumns, jsonAggFields)
	case "many":
		generateDialectManyMethod(buf, q, sqlField, dialect, typesPkg, jsonColumns, jsonAggFields)
	case "exec":
		generateDialectExecMethod(buf, q, sqlField, dialect, typesPkg)
	default:
		// Default to many for backward compatibility
		generateDialectManyMethod(buf, q, sqlField, dialect, typesPkg, jsonColumns, jsonAggFields)
	}
}

// generateDialectOneMethod generates a method that returns 0 or 1 row.
func generateDialectOneMethod(buf *bytes.Buffer, q CompiledQueryWithDialects, sqlField, dialect, typesPkg string, jsonColumns map[string]bool, jsonAggFields map[string]bool) {
	resultType := fmt.Sprintf("%s.%sResult", typesPkg, q.Name)
	paramsType := fmt.Sprintf("%s.%sParams", typesPkg, q.Name)

	// Get param order for this dialect (includes duplicates)
	paramOrder := getParamOccurrencesForDialect(q, dialect)

	buf.WriteString(fmt.Sprintf("// %s executes the query and returns at most one result.\n", q.Name))

	// Method signature
	if len(q.Params) > 0 {
		buf.WriteString(fmt.Sprintf("func (r *QueryRunner) %s(ctx context.Context, params %s) (*%s, error) {\n",
			q.Name, paramsType, resultType))
	} else {
		buf.WriteString(fmt.Sprintf("func (r *QueryRunner) %s(ctx context.Context) (*%s, error) {\n",
			q.Name, resultType))
	}

	// Build args using param occurrence order (handles duplicates correctly)
	if len(paramOrder) > 0 {
		buf.WriteString("\targs := []any{\n")
		for _, paramName := range paramOrder {
			buf.WriteString(fmt.Sprintf("\t\tparams.%s,\n", toPascalCase(paramName)))
		}
		buf.WriteString("\t}\n\n")
		buf.WriteString(fmt.Sprintf("\trow := r.db.QueryRowContext(ctx, r.%s, args...)\n", sqlField))
	} else {
		buf.WriteString(fmt.Sprintf("\trow := r.db.QueryRowContext(ctx, r.%s)\n", sqlField))
	}

	// Declare result and temp variables for JSON columns
	buf.WriteString(fmt.Sprintf("\n\tvar result %s\n", resultType))

	// For SQLite with JSON columns, we need sql.NullString to handle NULLs
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\tvar %sNull sql.NullString\n", toLowerCamelCase(res.Name)))
			}
		}
	}

	// For JSONAgg fields, we need intermediate variables to scan JSON then unmarshal
	if len(jsonAggFields) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonAggFields[fieldName] {
				if dialect == "sqlite" {
					buf.WriteString(fmt.Sprintf("\tvar %sJSON sql.NullString\n", toLowerCamelCase(res.Name)))
				} else {
					buf.WriteString(fmt.Sprintf("\tvar %sJSON []byte\n", toLowerCamelCase(res.Name)))
				}
			}
		}
	}

	// Scan
	buf.WriteString("\terr := row.Scan(\n")
	for _, res := range q.Results {
		fieldName := toPascalCase(res.Name)
		if jsonAggFields[fieldName] {
			// JSONAgg: scan to intermediate JSON variable
			buf.WriteString(fmt.Sprintf("\t\t&%sJSON,\n", toLowerCamelCase(res.Name)))
		} else if dialect == "sqlite" && jsonColumns[fieldName] {
			// SQLite: scan JSON to sql.NullString temp var
			buf.WriteString(fmt.Sprintf("\t\t&%sNull,\n", toLowerCamelCase(res.Name)))
		} else {
			buf.WriteString(fmt.Sprintf("\t\t&result.%s,\n", fieldName))
		}
	}
	buf.WriteString("\t)\n")
	buf.WriteString("\tif err == sql.ErrNoRows {\n")
	buf.WriteString("\t\treturn nil, nil\n")
	buf.WriteString("\t}\n")
	buf.WriteString("\tif err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")

	// For SQLite, convert JSON sql.NullString to []byte (nil if NULL)
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\tif %sNull.Valid {\n", toLowerCamelCase(res.Name)))
				buf.WriteString(fmt.Sprintf("\t\tresult.%s = []byte(%sNull.String)\n", fieldName, toLowerCamelCase(res.Name)))
				buf.WriteString("\t}\n")
			}
		}
	}

	// Unmarshal JSONAgg fields into typed slices, filtering out null entries
	// (MySQL/SQLite produce [null, {...}, null] for LEFT JOIN no-match rows)
	if len(jsonAggFields) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonAggFields[fieldName] {
				varName := toLowerCamelCase(res.Name) + "JSON"
				itemTypeName := fmt.Sprintf("%s.%s%sItem", typesPkg, q.Name, fieldName)
				if dialect == "sqlite" {
					buf.WriteString(fmt.Sprintf("\tif %s.Valid && %s.String != \"\" {\n", varName, varName))
					buf.WriteString(fmt.Sprintf("\t\tvar %sRaw []json.RawMessage\n", toLowerCamelCase(res.Name)))
					buf.WriteString(fmt.Sprintf("\t\tif err := json.Unmarshal([]byte(%s.String), &%sRaw); err != nil {\n", varName, toLowerCamelCase(res.Name)))
				} else {
					buf.WriteString(fmt.Sprintf("\tif len(%s) > 0 {\n", varName))
					buf.WriteString(fmt.Sprintf("\t\tvar %sRaw []json.RawMessage\n", toLowerCamelCase(res.Name)))
					buf.WriteString(fmt.Sprintf("\t\tif err := json.Unmarshal(%s, &%sRaw); err != nil {\n", varName, toLowerCamelCase(res.Name)))
				}
				buf.WriteString(fmt.Sprintf("\t\t\treturn nil, fmt.Errorf(\"unmarshal %s: %%w\", err)\n", fieldName))
				buf.WriteString("\t\t}\n")
				// Filter out null entries and unmarshal each item
				buf.WriteString(fmt.Sprintf("\t\tfor _, raw := range %sRaw {\n", toLowerCamelCase(res.Name)))
				buf.WriteString("\t\t\tif len(raw) == 0 || string(raw) == \"null\" {\n")
				buf.WriteString("\t\t\t\tcontinue\n")
				buf.WriteString("\t\t\t}\n")
				buf.WriteString(fmt.Sprintf("\t\t\tvar item %s\n", itemTypeName))
				buf.WriteString("\t\t\tif err := json.Unmarshal(raw, &item); err != nil {\n")
				buf.WriteString(fmt.Sprintf("\t\t\t\treturn nil, fmt.Errorf(\"unmarshal %s item: %%w\", err)\n", fieldName))
				buf.WriteString("\t\t\t}\n")
				buf.WriteString(fmt.Sprintf("\t\t\tresult.%s = append(result.%s, item)\n", fieldName, fieldName))
				buf.WriteString("\t\t}\n")
				buf.WriteString("\t}\n")
			}
		}
	}

	buf.WriteString("\treturn &result, nil\n")
	buf.WriteString("}\n\n")
}

// generateDialectManyMethod generates a method that returns 0 to N rows.
func generateDialectManyMethod(buf *bytes.Buffer, q CompiledQueryWithDialects, sqlField, dialect, typesPkg string, jsonColumns map[string]bool, jsonAggFields map[string]bool) {
	resultType := fmt.Sprintf("%s.%sResult", typesPkg, q.Name)
	paramsType := fmt.Sprintf("%s.%sParams", typesPkg, q.Name)

	// Get param order for this dialect (includes duplicates)
	paramOrder := getParamOccurrencesForDialect(q, dialect)

	buf.WriteString(fmt.Sprintf("// %s executes the query and returns all results.\n", q.Name))

	// Method signature
	if len(q.Params) > 0 {
		buf.WriteString(fmt.Sprintf("func (r *QueryRunner) %s(ctx context.Context, params %s) ([]%s, error) {\n",
			q.Name, paramsType, resultType))
	} else {
		buf.WriteString(fmt.Sprintf("func (r *QueryRunner) %s(ctx context.Context) ([]%s, error) {\n",
			q.Name, resultType))
	}

	// Build args using param occurrence order (handles duplicates correctly)
	if len(paramOrder) > 0 {
		buf.WriteString("\targs := []any{\n")
		for _, paramName := range paramOrder {
			buf.WriteString(fmt.Sprintf("\t\tparams.%s,\n", toPascalCase(paramName)))
		}
		buf.WriteString("\t}\n\n")
		buf.WriteString(fmt.Sprintf("\trows, err := r.db.QueryContext(ctx, r.%s, args...)\n", sqlField))
	} else {
		buf.WriteString(fmt.Sprintf("\trows, err := r.db.QueryContext(ctx, r.%s)\n", sqlField))
	}

	buf.WriteString("\tif err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")
	buf.WriteString("\tdefer rows.Close()\n\n")

	// Scan results
	buf.WriteString(fmt.Sprintf("\tvar results []%s\n", resultType))
	buf.WriteString("\tfor rows.Next() {\n")
	buf.WriteString(fmt.Sprintf("\t\tvar item %s\n", resultType))

	// For SQLite with JSON columns, we need sql.NullString to handle NULLs
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\t\tvar %sNull sql.NullString\n", toLowerCamelCase(res.Name)))
			}
		}
	}

	// For JSONAgg fields, we need intermediate variables to scan JSON then unmarshal
	if len(jsonAggFields) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonAggFields[fieldName] {
				if dialect == "sqlite" {
					buf.WriteString(fmt.Sprintf("\t\tvar %sJSON sql.NullString\n", toLowerCamelCase(res.Name)))
				} else {
					buf.WriteString(fmt.Sprintf("\t\tvar %sJSON []byte\n", toLowerCamelCase(res.Name)))
				}
			}
		}
	}

	buf.WriteString("\t\terr := rows.Scan(\n")
	for _, res := range q.Results {
		fieldName := toPascalCase(res.Name)
		if jsonAggFields[fieldName] {
			// JSONAgg: scan to intermediate JSON variable
			buf.WriteString(fmt.Sprintf("\t\t\t&%sJSON,\n", toLowerCamelCase(res.Name)))
		} else if dialect == "sqlite" && jsonColumns[fieldName] {
			// SQLite: scan JSON to sql.NullString temp var
			buf.WriteString(fmt.Sprintf("\t\t\t&%sNull,\n", toLowerCamelCase(res.Name)))
		} else {
			buf.WriteString(fmt.Sprintf("\t\t\t&item.%s,\n", fieldName))
		}
	}
	buf.WriteString("\t\t)\n")
	buf.WriteString("\t\tif err != nil {\n")
	buf.WriteString("\t\t\treturn nil, err\n")
	buf.WriteString("\t\t}\n")

	// For SQLite, convert JSON sql.NullString to []byte (nil if NULL)
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\t\tif %sNull.Valid {\n", toLowerCamelCase(res.Name)))
				buf.WriteString(fmt.Sprintf("\t\t\titem.%s = []byte(%sNull.String)\n", fieldName, toLowerCamelCase(res.Name)))
				buf.WriteString("\t\t}\n")
			}
		}
	}

	// Unmarshal JSONAgg fields into typed slices, filtering out null entries
	// (MySQL/SQLite produce [null, {...}, null] for LEFT JOIN no-match rows)
	if len(jsonAggFields) > 0 {
		for _, res := range q.Results {
			fieldName := toPascalCase(res.Name)
			if jsonAggFields[fieldName] {
				varName := toLowerCamelCase(res.Name) + "JSON"
				itemTypeName := fmt.Sprintf("%s.%s%sItem", typesPkg, q.Name, fieldName)
				if dialect == "sqlite" {
					buf.WriteString(fmt.Sprintf("\t\tif %s.Valid && %s.String != \"\" {\n", varName, varName))
					buf.WriteString(fmt.Sprintf("\t\t\tvar %sRaw []json.RawMessage\n", toLowerCamelCase(res.Name)))
					buf.WriteString(fmt.Sprintf("\t\t\tif err := json.Unmarshal([]byte(%s.String), &%sRaw); err != nil {\n", varName, toLowerCamelCase(res.Name)))
				} else {
					buf.WriteString(fmt.Sprintf("\t\tif len(%s) > 0 {\n", varName))
					buf.WriteString(fmt.Sprintf("\t\t\tvar %sRaw []json.RawMessage\n", toLowerCamelCase(res.Name)))
					buf.WriteString(fmt.Sprintf("\t\t\tif err := json.Unmarshal(%s, &%sRaw); err != nil {\n", varName, toLowerCamelCase(res.Name)))
				}
				buf.WriteString(fmt.Sprintf("\t\t\t\treturn nil, fmt.Errorf(\"unmarshal %s: %%w\", err)\n", fieldName))
				buf.WriteString("\t\t\t}\n")
				// Filter out null entries and unmarshal each item
				buf.WriteString(fmt.Sprintf("\t\t\tfor _, raw := range %sRaw {\n", toLowerCamelCase(res.Name)))
				buf.WriteString("\t\t\t\tif len(raw) == 0 || string(raw) == \"null\" {\n")
				buf.WriteString("\t\t\t\t\tcontinue\n")
				buf.WriteString("\t\t\t\t}\n")
				buf.WriteString(fmt.Sprintf("\t\t\t\tvar itemPart %s\n", itemTypeName))
				buf.WriteString("\t\t\t\tif err := json.Unmarshal(raw, &itemPart); err != nil {\n")
				buf.WriteString(fmt.Sprintf("\t\t\t\t\treturn nil, fmt.Errorf(\"unmarshal %s item: %%w\", err)\n", fieldName))
				buf.WriteString("\t\t\t\t}\n")
				buf.WriteString(fmt.Sprintf("\t\t\t\titem.%s = append(item.%s, itemPart)\n", fieldName, fieldName))
				buf.WriteString("\t\t\t}\n")
				buf.WriteString("\t\t}\n")
			}
		}
	}

	buf.WriteString("\t\tresults = append(results, item)\n")
	buf.WriteString("\t}\n\n")

	buf.WriteString("\tif err := rows.Err(); err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")
	buf.WriteString("\treturn results, nil\n")
	buf.WriteString("}\n\n")
}

// generateDialectExecMethod generates a method that executes without returning rows.
func generateDialectExecMethod(buf *bytes.Buffer, q CompiledQueryWithDialects, sqlField, dialect, typesPkg string) {
	paramsType := fmt.Sprintf("%s.%sParams", typesPkg, q.Name)

	// Get param order for this dialect (includes duplicates)
	paramOrder := getParamOccurrencesForDialect(q, dialect)

	buf.WriteString(fmt.Sprintf("// %s executes the query and returns the result.\n", q.Name))

	// Method signature
	if len(q.Params) > 0 {
		buf.WriteString(fmt.Sprintf("func (r *QueryRunner) %s(ctx context.Context, params %s) (sql.Result, error) {\n",
			q.Name, paramsType))
	} else {
		buf.WriteString(fmt.Sprintf("func (r *QueryRunner) %s(ctx context.Context) (sql.Result, error) {\n",
			q.Name))
	}

	// Build args using param occurrence order (handles duplicates correctly)
	if len(paramOrder) > 0 {
		buf.WriteString("\targs := []any{\n")
		for _, paramName := range paramOrder {
			buf.WriteString(fmt.Sprintf("\t\tparams.%s,\n", toPascalCase(paramName)))
		}
		buf.WriteString("\t}\n\n")
		buf.WriteString(fmt.Sprintf("\treturn r.db.ExecContext(ctx, r.%s, args...)\n", sqlField))
	} else {
		buf.WriteString(fmt.Sprintf("\treturn r.db.ExecContext(ctx, r.%s)\n", sqlField))
	}

	buf.WriteString("}\n\n")
}

// getParamOccurrencesForDialect returns the parameter occurrence order for a specific dialect.
func getParamOccurrencesForDialect(q CompiledQueryWithDialects, dialect string) []string {
	switch dialect {
	case "postgres":
		return q.ParamOccurrences.Postgres
	case "mysql":
		return q.ParamOccurrences.MySQL
	case "sqlite":
		return q.ParamOccurrences.SQLite
	default:
		// Fallback to postgres order
		return q.ParamOccurrences.Postgres
	}
}

// generateDialectCRUDMethods generates CRUD methods for a table with dialect-specific scanning.
func generateDialectCRUDMethods(buf *bytes.Buffer, table ddl.Table, opts CRUDOptions, dialect, typesPkg string) {
	analysis := AnalyzeTable(table)
	singular := toSingular(table.Name)
	singularPascal := toPascalCase(singular)
	pluralPascal := toPascalCase(table.Name)

	// Identify JSON columns
	jsonColumns := make(map[string]bool)
	for _, col := range table.Columns {
		mapping := MapColumnType(col)
		if mapping.GoType == "json.RawMessage" {
			jsonColumns[toPascalCase(col.Name)] = true
		}
	}

	// --- Get ---
	generateDialectGetMethod(buf, table, analysis, singularPascal, opts, dialect, typesPkg, jsonColumns)

	// --- List ---
	generateDialectListMethod(buf, table, analysis, singularPascal, pluralPascal, opts, dialect, typesPkg, jsonColumns)

	// --- Insert ---
	generateDialectInsertMethod(buf, table, analysis, singularPascal, opts, dialect, typesPkg)

	// --- Update ---
	generateDialectUpdateMethod(buf, table, analysis, singularPascal, opts, dialect, typesPkg)

	// --- Delete ---
	generateDialectDeleteMethod(buf, table, analysis, singularPascal, opts, dialect, typesPkg)

	// --- HardDelete ---
	if analysis.HasDeletedAt {
		generateDialectHardDeleteMethod(buf, table, analysis, singularPascal, opts, dialect, typesPkg)
	}
}

func generateDialectGetMethod(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions, dialect, typesPkg string, jsonColumns map[string]bool) {
	paramsType := fmt.Sprintf("%s.Get%sParams", typesPkg, singularPascal)
	resultType := fmt.Sprintf("%s.Get%sResult", typesPkg, singularPascal)

	buf.WriteString(fmt.Sprintf("// Get%s fetches a single %s by its identifier.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("func (r *QueryRunner) Get%s(ctx context.Context, params %s) (*%s, error) {\n",
		singularPascal, paramsType, resultType))

	// Build args list
	buf.WriteString("\tvar args []any\n")
	if analysis.HasPublicID {
		buf.WriteString("\targs = append(args, params.PublicID)\n")
	} else {
		buf.WriteString("\targs = append(args, params.ID)\n")
	}
	if opts.ScopeColumn != "" {
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(opts.ScopeColumn)))
	}

	buf.WriteString(fmt.Sprintf("\n\trow := r.db.QueryRowContext(ctx, r.get%sSQL, args...)\n", singularPascal))
	buf.WriteString(fmt.Sprintf("\n\tvar result %s\n", resultType))

	// For SQLite with JSON columns, use sql.NullString to handle NULLs
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, col := range analysis.ResultColumns {
			fieldName := toPascalCase(col.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\tvar %sNull sql.NullString\n", toLowerCamelCase(col.Name)))
			}
		}
	}

	buf.WriteString("\terr := row.Scan(\n")

	// Scan result columns
	for _, col := range analysis.ResultColumns {
		fieldName := toPascalCase(col.Name)
		if dialect == "sqlite" && jsonColumns[fieldName] {
			buf.WriteString(fmt.Sprintf("\t\t&%sNull,\n", toLowerCamelCase(col.Name)))
		} else {
			buf.WriteString(fmt.Sprintf("\t\t&result.%s,\n", fieldName))
		}
	}
	buf.WriteString("\t)\n")
	buf.WriteString("\tif err == sql.ErrNoRows {\n")
	buf.WriteString("\t\treturn nil, nil\n")
	buf.WriteString("\t}\n")
	buf.WriteString("\tif err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")

	// For SQLite, convert JSON sql.NullString to []byte (nil if NULL)
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, col := range analysis.ResultColumns {
			fieldName := toPascalCase(col.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\tif %sNull.Valid {\n", toLowerCamelCase(col.Name)))
				buf.WriteString(fmt.Sprintf("\t\tresult.%s = []byte(%sNull.String)\n", fieldName, toLowerCamelCase(col.Name)))
				buf.WriteString("\t}\n")
			}
		}
	}

	buf.WriteString("\treturn &result, nil\n")
	buf.WriteString("}\n\n")
}

func generateDialectListMethod(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal, pluralPascal string, opts CRUDOptions, dialect, typesPkg string, jsonColumns map[string]bool) {
	// Check if cursor pagination is supported
	supportsCursor := analysis.HasCreatedAt && analysis.HasPublicID

	if supportsCursor {
		generateDialectListMethodWithCursor(buf, table, analysis, singularPascal, pluralPascal, opts, dialect, typesPkg, jsonColumns)
	} else {
		generateDialectListMethodWithOffset(buf, table, analysis, singularPascal, pluralPascal, opts, dialect, typesPkg, jsonColumns)
	}
}

// generateDialectListMethodWithCursor generates list method with cursor-based pagination
func generateDialectListMethodWithCursor(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal, pluralPascal string, opts CRUDOptions, dialect, typesPkg string, jsonColumns map[string]bool) {
	paramsType := fmt.Sprintf("%s.List%sParams", typesPkg, pluralPascal)
	resultType := fmt.Sprintf("%s.List%sResult", typesPkg, pluralPascal)
	itemType := fmt.Sprintf("%s.List%sItem", typesPkg, pluralPascal)
	cursorType := fmt.Sprintf("%s.List%sCursor", typesPkg, pluralPascal)

	buf.WriteString(fmt.Sprintf("// List%s fetches a paginated list of %s using cursor-based pagination.\n", pluralPascal, table.Name))
	buf.WriteString(fmt.Sprintf("func (r *QueryRunner) List%s(ctx context.Context, params %s) (*%s, error) {\n",
		pluralPascal, paramsType, resultType))

	// Build SQL dynamically based on optional params
	buf.WriteString("\t// Build SQL with optional cursor and filter conditions\n")
	buf.WriteString(fmt.Sprintf("\tsql := r.list%sSQL\n", pluralPascal))
	buf.WriteString("\tvar args []any\n")

	// Add scope column first if configured
	if opts.ScopeColumn != "" {
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(opts.ScopeColumn)))
	}

	buf.WriteString("\n")

	// Build WHERE clause additions for filters and cursor
	buf.WriteString("\t// Add optional filter and cursor conditions\n")
	buf.WriteString("\tvar whereClauses []string\n")

	// CreatedAfter filter
	buf.WriteString("\tif params.CreatedAfter != nil {\n")
	buf.WriteString(fmt.Sprintf("\t\twhereClauses = append(whereClauses, %s)\n", formatDynamicCondition("created_at", ">=", dialect)))
	buf.WriteString("\t\targs = append(args, *params.CreatedAfter)\n")
	buf.WriteString("\t}\n")

	// CreatedBefore filter
	buf.WriteString("\tif params.CreatedBefore != nil {\n")
	buf.WriteString(fmt.Sprintf("\t\twhereClauses = append(whereClauses, %s)\n", formatDynamicCondition("created_at", "<", dialect)))
	buf.WriteString("\t\targs = append(args, *params.CreatedBefore)\n")
	buf.WriteString("\t}\n")

	// Cursor condition (keyset pagination)
	buf.WriteString("\tif params.Cursor != nil {\n")
	// The cursor condition: (created_at < ? OR (created_at = ? AND id < (SELECT id FROM table WHERE public_id = ?)))
	cursorCondition := formatCursorCondition(table.Name, dialect)
	buf.WriteString(fmt.Sprintf("\t\twhereClauses = append(whereClauses, %s)\n", cursorCondition))
	buf.WriteString("\t\targs = append(args, params.Cursor.CreatedAt, params.Cursor.CreatedAt, params.Cursor.PublicID)\n")
	buf.WriteString("\t}\n")

	// Insert WHERE clauses into SQL
	buf.WriteString("\n\t// Insert additional WHERE conditions before ORDER BY\n")
	buf.WriteString("\tif len(whereClauses) > 0 {\n")
	buf.WriteString("\t\torderIdx := strings.Index(sql, \" ORDER BY\")\n")
	buf.WriteString("\t\tif orderIdx > 0 {\n")
	buf.WriteString("\t\t\tsql = sql[:orderIdx] + \" AND \" + strings.Join(whereClauses, \" AND \") + sql[orderIdx:]\n")
	buf.WriteString("\t\t}\n")
	buf.WriteString("\t}\n")

	// Fetch limit + 1 to detect if more pages exist
	buf.WriteString("\n\t// Fetch limit + 1 to detect if more pages exist\n")
	buf.WriteString("\targs = append(args, params.Limit + 1)\n")

	buf.WriteString("\n\trows, err := r.db.QueryContext(ctx, sql, args...)\n")
	buf.WriteString("\tif err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")
	buf.WriteString("\tdefer rows.Close()\n")

	buf.WriteString(fmt.Sprintf("\n\tvar items []%s\n", itemType))
	buf.WriteString("\tfor rows.Next() {\n")
	buf.WriteString(fmt.Sprintf("\t\tvar item %s\n", itemType))

	// For SQLite with JSON columns, use sql.NullString to handle NULLs
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, col := range analysis.ResultColumns {
			if col.Name == "updated_at" {
				continue
			}
			fieldName := toPascalCase(col.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\t\tvar %sNull sql.NullString\n", toLowerCamelCase(col.Name)))
			}
		}
	}

	buf.WriteString("\t\terr := rows.Scan(\n")

	// Scan result columns (excluding updated_at)
	for _, col := range analysis.ResultColumns {
		if col.Name == "updated_at" {
			continue
		}
		fieldName := toPascalCase(col.Name)
		if dialect == "sqlite" && jsonColumns[fieldName] {
			buf.WriteString(fmt.Sprintf("\t\t\t&%sNull,\n", toLowerCamelCase(col.Name)))
		} else {
			buf.WriteString(fmt.Sprintf("\t\t\t&item.%s,\n", fieldName))
		}
	}
	buf.WriteString("\t\t)\n")
	buf.WriteString("\t\tif err != nil {\n")
	buf.WriteString("\t\t\treturn nil, err\n")
	buf.WriteString("\t\t}\n")

	// For SQLite, convert JSON sql.NullString to []byte (nil if NULL)
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, col := range analysis.ResultColumns {
			if col.Name == "updated_at" {
				continue
			}
			fieldName := toPascalCase(col.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\t\tif %sNull.Valid {\n", toLowerCamelCase(col.Name)))
				buf.WriteString(fmt.Sprintf("\t\t\titem.%s = []byte(%sNull.String)\n", fieldName, toLowerCamelCase(col.Name)))
				buf.WriteString("\t\t}\n")
			}
		}
	}

	buf.WriteString("\t\titems = append(items, item)\n")
	buf.WriteString("\t}\n")

	buf.WriteString("\tif err := rows.Err(); err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")

	// Build result with NextCursor
	buf.WriteString("\n\t// Build result with NextCursor if more pages exist\n")
	buf.WriteString(fmt.Sprintf("\tresult := &%s{}\n", resultType))
	buf.WriteString("\tif len(items) > params.Limit {\n")
	buf.WriteString("\t\t// More pages exist - trim to requested limit and build cursor\n")
	buf.WriteString("\t\tresult.Items = items[:params.Limit]\n")
	buf.WriteString("\t\tlastItem := result.Items[len(result.Items)-1]\n")
	buf.WriteString(fmt.Sprintf("\t\tresult.NextCursor = &%s{\n", cursorType))
	buf.WriteString("\t\t\tCreatedAt: lastItem.CreatedAt,\n")
	buf.WriteString("\t\t\tPublicID:  lastItem.PublicId,\n")
	buf.WriteString("\t\t}\n")
	buf.WriteString("\t} else {\n")
	buf.WriteString("\t\tresult.Items = items\n")
	buf.WriteString("\t}\n")

	buf.WriteString("\treturn result, nil\n")
	buf.WriteString("}\n\n")
}

// formatDynamicCondition returns a Go string literal for a dynamic WHERE condition
func formatDynamicCondition(column, operator, dialect string) string {
	switch dialect {
	case "mysql":
		return fmt.Sprintf("`%s` + ` %s ?`", column, operator)
	default:
		// Use backtick for the outer Go string to avoid escaping inner double quotes
		return fmt.Sprintf("`\"%s\" %s ?`", column, operator)
	}
}

// formatCursorCondition returns a Go string literal for the cursor keyset condition
func formatCursorCondition(tableName, dialect string) string {
	switch dialect {
	case "mysql":
		// For MySQL, use backticks for identifier quoting
		return fmt.Sprintf("`(` + \"`created_at`\" + ` < ? OR (` + \"`created_at`\" + ` = ? AND ` + \"`id`\" + ` < (SELECT ` + \"`id`\" + ` FROM ` + \"`%s`\" + ` WHERE ` + \"`public_id`\" + ` = ?)))`", tableName)
	default:
		// For Postgres/SQLite, use double quotes for identifier quoting
		// Use backticks for the Go string to contain the SQL
		return fmt.Sprintf("`(\"created_at\" < ? OR (\"created_at\" = ? AND \"id\" < (SELECT \"id\" FROM \"%s\" WHERE \"public_id\" = ?)))`", tableName)
	}
}

// generateDialectListMethodWithOffset generates list method with offset-based pagination (fallback)
func generateDialectListMethodWithOffset(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal, pluralPascal string, opts CRUDOptions, dialect, typesPkg string, jsonColumns map[string]bool) {
	paramsType := fmt.Sprintf("%s.List%sParams", typesPkg, pluralPascal)
	resultType := fmt.Sprintf("%s.List%sResult", typesPkg, pluralPascal)

	buf.WriteString(fmt.Sprintf("// List%s fetches a paginated list of %s.\n", pluralPascal, table.Name))
	buf.WriteString(fmt.Sprintf("func (r *QueryRunner) List%s(ctx context.Context, params %s) ([]%s, error) {\n",
		pluralPascal, paramsType, resultType))

	// Build args list
	buf.WriteString("\tvar args []any\n")
	if opts.ScopeColumn != "" {
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(opts.ScopeColumn)))
	}
	buf.WriteString("\targs = append(args, params.Limit, params.Offset)\n")

	buf.WriteString(fmt.Sprintf("\n\trows, err := r.db.QueryContext(ctx, r.list%sSQL, args...)\n", pluralPascal))
	buf.WriteString("\tif err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")
	buf.WriteString("\tdefer rows.Close()\n")

	buf.WriteString(fmt.Sprintf("\n\tvar results []%s\n", resultType))
	buf.WriteString("\tfor rows.Next() {\n")
	buf.WriteString(fmt.Sprintf("\t\tvar item %s\n", resultType))

	// For SQLite with JSON columns, use sql.NullString to handle NULLs
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, col := range analysis.ResultColumns {
			if col.Name == "updated_at" {
				continue
			}
			fieldName := toPascalCase(col.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\t\tvar %sNull sql.NullString\n", toLowerCamelCase(col.Name)))
			}
		}
	}

	buf.WriteString("\t\terr := rows.Scan(\n")

	// Scan result columns (excluding updated_at)
	for _, col := range analysis.ResultColumns {
		if col.Name == "updated_at" {
			continue
		}
		fieldName := toPascalCase(col.Name)
		if dialect == "sqlite" && jsonColumns[fieldName] {
			buf.WriteString(fmt.Sprintf("\t\t\t&%sNull,\n", toLowerCamelCase(col.Name)))
		} else {
			buf.WriteString(fmt.Sprintf("\t\t\t&item.%s,\n", fieldName))
		}
	}
	buf.WriteString("\t\t)\n")
	buf.WriteString("\t\tif err != nil {\n")
	buf.WriteString("\t\t\treturn nil, err\n")
	buf.WriteString("\t\t}\n")

	// For SQLite, convert JSON sql.NullString to []byte (nil if NULL)
	if dialect == "sqlite" && len(jsonColumns) > 0 {
		for _, col := range analysis.ResultColumns {
			if col.Name == "updated_at" {
				continue
			}
			fieldName := toPascalCase(col.Name)
			if jsonColumns[fieldName] {
				buf.WriteString(fmt.Sprintf("\t\tif %sNull.Valid {\n", toLowerCamelCase(col.Name)))
				buf.WriteString(fmt.Sprintf("\t\t\titem.%s = []byte(%sNull.String)\n", fieldName, toLowerCamelCase(col.Name)))
				buf.WriteString("\t\t}\n")
			}
		}
	}

	buf.WriteString("\t\tresults = append(results, item)\n")
	buf.WriteString("\t}\n")

	buf.WriteString("\tif err := rows.Err(); err != nil {\n")
	buf.WriteString("\t\treturn nil, err\n")
	buf.WriteString("\t}\n")
	buf.WriteString("\treturn results, nil\n")
	buf.WriteString("}\n\n")
}

func generateDialectInsertMethod(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions, dialect, typesPkg string) {
	paramsType := fmt.Sprintf("%s.Insert%sParams", typesPkg, singularPascal)

	// Determine return type
	returnType := "error"
	if analysis.HasPublicID {
		returnType = "(string, error)"
	}

	buf.WriteString(fmt.Sprintf("// Insert%s inserts a new %s and returns its public ID.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("func (r *QueryRunner) Insert%s(ctx context.Context, params %s) %s {\n",
		singularPascal, paramsType, returnType))

	// Generate public_id if needed
	if analysis.HasPublicID {
		buf.WriteString("\tpublicID := nanoid.New()\n\n")
	}

	// Build args list
	buf.WriteString("\tvar args []any\n")
	if analysis.HasPublicID {
		buf.WriteString("\targs = append(args, publicID)\n")
	}
	if opts.ScopeColumn != "" {
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(opts.ScopeColumn)))
	}
	for _, col := range analysis.UserColumns {
		if opts.ScopeColumn != "" && col.Name == opts.ScopeColumn {
			continue
		}
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(col.Name)))
	}

	buf.WriteString("\n")

	if analysis.HasPublicID {
		// Handle dialect-specific RETURNING behavior
		if dialect == "mysql" {
			buf.WriteString(fmt.Sprintf("\t_, err := r.db.ExecContext(ctx, r.insert%sSQL, args...)\n", singularPascal))
			buf.WriteString("\tif err != nil {\n")
			buf.WriteString("\t\treturn \"\", err\n")
			buf.WriteString("\t}\n")
			buf.WriteString("\treturn publicID, nil\n")
		} else {
			// Postgres/SQLite: Use RETURNING
			buf.WriteString("\tvar returnedID string\n")
			buf.WriteString(fmt.Sprintf("\terr := r.db.QueryRowContext(ctx, r.insert%sSQL, args...).Scan(&returnedID)\n", singularPascal))
			buf.WriteString("\tif err != nil {\n")
			buf.WriteString("\t\treturn \"\", err\n")
			buf.WriteString("\t}\n")
			buf.WriteString("\treturn returnedID, nil\n")
		}
	} else {
		buf.WriteString(fmt.Sprintf("\t_, err := r.db.ExecContext(ctx, r.insert%sSQL, args...)\n", singularPascal))
		buf.WriteString("\treturn err\n")
	}

	buf.WriteString("}\n\n")
}

func generateDialectUpdateMethod(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions, dialect, typesPkg string) {
	paramsType := fmt.Sprintf("%s.Update%sParams", typesPkg, singularPascal)

	buf.WriteString(fmt.Sprintf("// Update%s updates an existing %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("func (r *QueryRunner) Update%s(ctx context.Context, params %s) error {\n",
		singularPascal, paramsType))

	// Build args list: SET values first, then WHERE values
	buf.WriteString("\tvar args []any\n")

	// SET clause args (user columns, excluding scope)
	for _, col := range analysis.UserColumns {
		if opts.ScopeColumn != "" && col.Name == opts.ScopeColumn {
			continue
		}
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(col.Name)))
	}

	// WHERE clause args
	if analysis.HasPublicID {
		buf.WriteString("\targs = append(args, params.PublicID)\n")
	} else {
		buf.WriteString("\targs = append(args, params.ID)\n")
	}
	if opts.ScopeColumn != "" {
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(opts.ScopeColumn)))
	}

	buf.WriteString(fmt.Sprintf("\n\t_, err := r.db.ExecContext(ctx, r.update%sSQL, args...)\n", singularPascal))
	buf.WriteString("\treturn err\n")
	buf.WriteString("}\n\n")
}

func generateDialectDeleteMethod(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions, dialect, typesPkg string) {
	paramsType := fmt.Sprintf("%s.Delete%sParams", typesPkg, singularPascal)

	buf.WriteString(fmt.Sprintf("// Delete%s soft-deletes a %s (or hard-deletes if no deleted_at column).\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("func (r *QueryRunner) Delete%s(ctx context.Context, params %s) error {\n",
		singularPascal, paramsType))

	// Build args list
	buf.WriteString("\tvar args []any\n")
	if analysis.HasPublicID {
		buf.WriteString("\targs = append(args, params.PublicID)\n")
	} else {
		buf.WriteString("\targs = append(args, params.ID)\n")
	}
	if opts.ScopeColumn != "" {
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(opts.ScopeColumn)))
	}

	buf.WriteString(fmt.Sprintf("\n\t_, err := r.db.ExecContext(ctx, r.delete%sSQL, args...)\n", singularPascal))
	buf.WriteString("\treturn err\n")
	buf.WriteString("}\n\n")
}

func generateDialectHardDeleteMethod(buf *bytes.Buffer, table ddl.Table, analysis TableAnalysis, singularPascal string, opts CRUDOptions, dialect, typesPkg string) {
	paramsType := fmt.Sprintf("%s.HardDelete%sParams", typesPkg, singularPascal)

	buf.WriteString(fmt.Sprintf("// HardDelete%s permanently deletes a %s.\n", singularPascal, toSingular(table.Name)))
	buf.WriteString(fmt.Sprintf("func (r *QueryRunner) HardDelete%s(ctx context.Context, params %s) error {\n",
		singularPascal, paramsType))

	// Build args list
	buf.WriteString("\tvar args []any\n")
	if analysis.HasPublicID {
		buf.WriteString("\targs = append(args, params.PublicID)\n")
	} else {
		buf.WriteString("\targs = append(args, params.ID)\n")
	}
	if opts.ScopeColumn != "" {
		buf.WriteString(fmt.Sprintf("\targs = append(args, params.%s)\n", toPascalCase(opts.ScopeColumn)))
	}

	buf.WriteString(fmt.Sprintf("\n\t_, err := r.db.ExecContext(ctx, r.hardDelete%sSQL, args...)\n", singularPascal))
	buf.WriteString("\treturn err\n")
	buf.WriteString("}\n\n")
}
